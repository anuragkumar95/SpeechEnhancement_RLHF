{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8944d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.conformer import ConformerBlock\n",
    "import torch\n",
    "#import torch.nn as nn\n",
    "\n",
    "#import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# source: https://github.com/lucidrains/conformer/blob/master/conformer/conformer.py\n",
    "# helper functions\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "\n",
    "def calc_same_padding(kernel_size):\n",
    "    pad = kernel_size // 2\n",
    "    return (pad, pad - (kernel_size + 1) % 2)\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * x.sigmoid()\n",
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, gate = x.chunk(2, dim=self.dim)\n",
    "        return out * gate.sigmoid()\n",
    "\n",
    "\n",
    "class DepthWiseConv1d(nn.Module):\n",
    "    def __init__(self, chan_in, chan_out, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.padding = padding\n",
    "        self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups=chan_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, self.padding)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# attention, feedforward, and conv module\n",
    "\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, scale, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) * self.scale\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0, max_pos_emb=512):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.max_pos_emb = max_pos_emb\n",
    "        self.rel_pos_emb = nn.Embedding(2 * max_pos_emb + 1, dim_head)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, context_mask=None):\n",
    "        n, device, h, max_pos_emb, has_context = (\n",
    "            x.shape[-2],\n",
    "            x.device,\n",
    "            self.heads,\n",
    "            self.max_pos_emb,\n",
    "            exists(context),\n",
    "        )\n",
    "        context = default(context, x)\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim=-1))\n",
    "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n",
    "\n",
    "        dots = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
    "\n",
    "        # shaw's relative positional embedding\n",
    "        seq = torch.arange(n, device=device)\n",
    "        dist = rearrange(seq, \"i -> i ()\") - rearrange(seq, \"j -> () j\")\n",
    "        dist = dist.clamp(-max_pos_emb, max_pos_emb) + max_pos_emb\n",
    "        rel_pos_emb = self.rel_pos_emb(dist).to(q)\n",
    "        pos_attn = einsum(\"b h n d, n r d -> b h n r\", q, rel_pos_emb) * self.scale\n",
    "        dots = dots + pos_attn\n",
    "\n",
    "        if exists(mask) or exists(context_mask):\n",
    "            mask = default(mask, lambda: torch.ones(*x.shape[:2], device=device))\n",
    "            context_mask = (\n",
    "                default(context_mask, mask)\n",
    "                if not has_context\n",
    "                else default(\n",
    "                    context_mask, lambda: torch.ones(*context.shape[:2], device=device)\n",
    "                )\n",
    "            )\n",
    "            mask_value = -torch.finfo(dots.dtype).max\n",
    "            mask = rearrange(mask, \"b i -> b () i ()\") * rearrange(\n",
    "                context_mask, \"b j -> b () () j\"\n",
    "            )\n",
    "            dots.masked_fill_(~mask, mask_value)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.to_out(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ConformerConvModule(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, causal=False, expansion_factor=2, kernel_size=31, dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        inner_dim = dim * expansion_factor\n",
    "        padding = calc_same_padding(kernel_size) if not causal else (kernel_size - 1, 0)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Rearrange(\"b n c -> b c n\"),\n",
    "            nn.Conv1d(dim, inner_dim * 2, 1),\n",
    "            GLU(dim=1),\n",
    "            DepthWiseConv1d(\n",
    "                inner_dim, inner_dim, kernel_size=kernel_size, padding=padding\n",
    "            ),\n",
    "            nn.BatchNorm1d(inner_dim) if not causal else nn.Identity(),\n",
    "            Swish(),\n",
    "            nn.Conv1d(inner_dim, dim, 1),\n",
    "            Rearrange(\"b c n -> b n c\"),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Conformer Block\n",
    "\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        ff_mult=4,\n",
    "        conv_expansion_factor=2,\n",
    "        conv_kernel_size=31,\n",
    "        attn_dropout=0.0,\n",
    "        ff_dropout=0.0,\n",
    "        conv_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ff1 = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout)\n",
    "        self.attn = Attention(\n",
    "            dim=dim, dim_head=dim_head, heads=heads, dropout=attn_dropout\n",
    "        )\n",
    "        self.conv = ConformerConvModule(\n",
    "            dim=dim,\n",
    "            causal=False,\n",
    "            expansion_factor=conv_expansion_factor,\n",
    "            kernel_size=conv_kernel_size,\n",
    "            dropout=conv_dropout,\n",
    "        )\n",
    "        self.ff2 = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout)\n",
    "\n",
    "        self.attn = PreNorm(dim, self.attn)\n",
    "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1))\n",
    "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2))\n",
    "\n",
    "        self.post_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.ff1(x) + x\n",
    "        x = self.attn(x, mask=mask) + x\n",
    "        x = self.conv(x) + x\n",
    "        x = self.ff2(x) + x\n",
    "        x = self.post_norm(x)\n",
    "        return x\n",
    "\n",
    "class DilatedDenseNet(nn.Module):\n",
    "    def __init__(self, depth=4, in_channels=64):\n",
    "        super(DilatedDenseNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.in_channels = in_channels\n",
    "        self.pad = nn.ConstantPad2d((1, 1, 1, 0), value=0.0)\n",
    "        self.twidth = 2\n",
    "        self.kernel_size = (self.twidth, 3)\n",
    "        for i in range(self.depth):\n",
    "            dil = 2**i\n",
    "            pad_length = self.twidth + (dil - 1) * (self.twidth - 1) - 1\n",
    "            setattr(\n",
    "                self,\n",
    "                \"pad{}\".format(i + 1),\n",
    "                nn.ConstantPad2d((1, 1, pad_length, 0), value=0.0),\n",
    "            )\n",
    "            setattr(\n",
    "                self,\n",
    "                \"conv{}\".format(i + 1),\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels * (i + 1),\n",
    "                    self.in_channels,\n",
    "                    kernel_size=self.kernel_size,\n",
    "                    dilation=(dil, 1),\n",
    "                ),\n",
    "            )\n",
    "            setattr(\n",
    "                self,\n",
    "                \"norm{}\".format(i + 1),\n",
    "                nn.InstanceNorm2d(in_channels, affine=True),\n",
    "            )\n",
    "            setattr(self, \"prelu{}\".format(i + 1), nn.PReLU(self.in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        for i in range(self.depth):\n",
    "            out = getattr(self, \"pad{}\".format(i + 1))(skip)\n",
    "            out = getattr(self, \"conv{}\".format(i + 1))(out)\n",
    "            out = getattr(self, \"norm{}\".format(i + 1))(out)\n",
    "            out = getattr(self, \"prelu{}\".format(i + 1))(out)\n",
    "            skip = torch.cat([out, skip], dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseEncoder(nn.Module):\n",
    "    def __init__(self, in_channel, channels=64):\n",
    "        super(DenseEncoder, self).__init__()\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, channels, (1, 1), (1, 1)),\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "            nn.PReLU(channels),\n",
    "        )\n",
    "        self.dilated_dense = DilatedDenseNet(depth=4, in_channels=channels)\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, (1, 3), (1, 2), padding=(0, 1)),\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "            nn.PReLU(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.dilated_dense(x)\n",
    "        x = self.conv_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TSCB(nn.Module):\n",
    "    def __init__(self, num_channel=64, out_channel=2):\n",
    "        super(TSCB, self).__init__()\n",
    "        self.time_conformer = ConformerBlock(\n",
    "            dim=num_channel,\n",
    "            dim_head=num_channel // 4,\n",
    "            heads=4,\n",
    "            conv_kernel_size=31,\n",
    "            attn_dropout=0.2,\n",
    "            ff_dropout=0.2,\n",
    "        )\n",
    "        self.freq_conformer = ConformerBlock(\n",
    "            dim=num_channel,\n",
    "            dim_head=num_channel // 4,\n",
    "            heads=4,\n",
    "            conv_kernel_size=31,\n",
    "            attn_dropout=0.2,\n",
    "            ff_dropout=0.2,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        b, c, t, f = x_in.size()\n",
    "        x_t = x_in.permute(0, 3, 2, 1).contiguous().view(b * f, t, c)\n",
    "        x_t = self.time_conformer(x_t) + x_t\n",
    "        x_f = x_t.view(b, f, t, c).permute(0, 2, 1, 3).contiguous().view(b * t, f, c)\n",
    "        x_f = self.freq_conformer(x_f) + x_f\n",
    "        x_f = x_f.view(b, t, f, c).permute(0, 3, 1, 2)\n",
    "        return x_f\n",
    "\n",
    "\n",
    "class SPConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, r=1):\n",
    "        super(SPConvTranspose2d, self).__init__()\n",
    "        self.pad1 = nn.ConstantPad2d((1, 1, 0, 0), value=0.0)\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels * r, kernel_size=kernel_size, stride=(1, 1)\n",
    "        )\n",
    "        self.r = r\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad1(x)\n",
    "        out = self.conv(x)\n",
    "        batch_size, nchannels, H, W = out.shape\n",
    "        out = out.view((batch_size, self.r, nchannels // self.r, H, W))\n",
    "        out = out.permute(0, 2, 3, 4, 1)\n",
    "        out = out.contiguous().view((batch_size, nchannels // self.r, H, -1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaskDecoder(nn.Module):\n",
    "    def __init__(self, num_features, num_channel=64, out_channel=1, signal_window=51, gpu_id=None):\n",
    "        super(MaskDecoder, self).__init__()\n",
    "        self.dense_block = DilatedDenseNet(depth=4, in_channels=num_channel)\n",
    "        self.sub_pixel = SPConvTranspose2d(num_channel, num_channel, (1, 3), 2)\n",
    "        self.conv_1 = nn.Conv2d(num_channel, out_channel, (1, 2))\n",
    "        self.norm = nn.InstanceNorm2d(out_channel, affine=True)\n",
    "        self.prelu = nn.PReLU(out_channel)\n",
    "        self.final_conv = nn.Conv2d(out_channel, out_channel, (1, 1))\n",
    "        self.prelu_out = nn.PReLU(num_features, init=-0.25)\n",
    "        self.relu = nn.ReLU()\n",
    "        #Predict mask for the middle frame of window\n",
    "        self.out_mu = nn.Linear(signal_window, 1)\n",
    "        self.out_sigma = nn.Linear(signal_window, 1)\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "    def sample(self, mu, sigma):\n",
    "        x = mu + sigma * self.N.sample(mu.shape).to(self.gpu_id)\n",
    "        x = self.prelu_out(x)\n",
    "        return x.permute(0, 2, 1).unsqueeze(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_block(x)\n",
    "        x = self.sub_pixel(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.prelu(self.norm(x))\n",
    "        x = self.final_conv(x).permute(0, 3, 2, 1).squeeze(-1)\n",
    "        #Predict mask for the middle frame of the input window\n",
    "        #as we learn a distribution\n",
    "        x_mu = self.out_mu(x)\n",
    "        x_sigma = self.relu(self.out_sigma(x))\n",
    "        return x_mu, x_sigma\n",
    "\n",
    "class ComplexDecoder(nn.Module):\n",
    "    def __init__(self, num_channel=64, signal_window=51, gpu_id=None):\n",
    "        super(ComplexDecoder, self).__init__()\n",
    "        self.dense_block = DilatedDenseNet(depth=4, in_channels=num_channel)\n",
    "        self.sub_pixel = SPConvTranspose2d(num_channel, num_channel, (1, 3), 2)\n",
    "        self.prelu = nn.PReLU(num_channel)\n",
    "        self.norm = nn.InstanceNorm2d(num_channel, affine=True)\n",
    "        self.conv = nn.Conv2d(num_channel, 2, (1, 2))\n",
    "        self.out_mu = nn.Linear(signal_window, 1)\n",
    "        self.out_sigma = nn.Linear(signal_window, 1)\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.gpu_id = gpu_id\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def sample(self, mu, sigma):\n",
    "        x = mu + sigma * self.N.sample(mu.shape).to(self.gpu_id)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_block(x)\n",
    "        x = self.sub_pixel(x)\n",
    "        x = self.prelu(self.norm(x))\n",
    "        x = self.conv(x)\n",
    "        #Predict mask for the middle frame of the input window\n",
    "        #as we learn a distribution\n",
    "        x_mu = self.out_mu(x.permute(0,1,3,2))\n",
    "        x_sigma = self.relu(self.out_sigma(x.permute(0,1,3,2)))\n",
    "        return x_mu, x_sigma\n",
    "\n",
    "class TSCNet(nn.Module):\n",
    "    def __init__(self, num_channel=64, num_features=201, win_len=51, mag_only=False, gpu_id=None):\n",
    "        super(TSCNet, self).__init__()\n",
    "        if mag_only:\n",
    "            in_channels=1\n",
    "        else:\n",
    "            in_channels=3\n",
    "        self.dense_encoder = DenseEncoder(in_channel=in_channels, channels=num_channel)\n",
    "\n",
    "        self.TSCB_1 = TSCB(num_channel=num_channel)\n",
    "        self.TSCB_2 = TSCB(num_channel=num_channel)\n",
    "        self.TSCB_3 = TSCB(num_channel=num_channel)\n",
    "        self.TSCB_4 = TSCB(num_channel=num_channel)\n",
    "\n",
    "        self.mask_decoder = MaskDecoder(\n",
    "            num_features, num_channel=num_channel, out_channel=1, signal_window=win_len, gpu_id=gpu_id\n",
    "        )\n",
    "        self.complex_decoder = ComplexDecoder(num_channel=num_channel, signal_window=win_len, gpu_id=gpu_id)\n",
    "\n",
    "    def forward(self, x, mag_only=False):\n",
    "        mag = torch.sqrt(x[:, 0, :, :] ** 2 + x[:, 1, :, :] ** 2).unsqueeze(1)\n",
    "    \n",
    "        win_len = mag.shape[2]\n",
    "        \n",
    "        noisy_phase = torch.angle(\n",
    "            torch.complex(x[:, 0, :, :], x[:, 1, :, :])\n",
    "        ).unsqueeze(1)\n",
    "        if mag_only:\n",
    "            x_in = mag\n",
    "        else:\n",
    "            x_in = torch.cat([mag, x], dim=1)\n",
    "       \n",
    "        out_1 = self.dense_encoder(x_in)\n",
    "        out_2 = self.TSCB_1(out_1)\n",
    "        out_3 = self.TSCB_2(out_2)\n",
    "        out_4 = self.TSCB_3(out_3)\n",
    "        out_5 = self.TSCB_4(out_4)\n",
    "\n",
    "        #Sample mask from the output distribution k times and take the average.\n",
    "        mask_mu, mask_sigma = self.mask_decoder(out_5)\n",
    "        mask = self.mask_decoder.sample(mask_mu, mask_sigma)\n",
    "        \n",
    "        #Output mask is for the middle frame of the window\n",
    "        out_mag = mask * mag[:, :, win_len//2 + 1, :].unsqueeze(2)\n",
    "        mag_real = (out_mag * torch.cos(noisy_phase[:, :, win_len//2 + 1, :].unsqueeze(2))).permute(0, 1, 3, 2)\n",
    "        mag_imag = (out_mag * torch.sin(noisy_phase[:, :, win_len//2 + 1, :].unsqueeze(2))).permute(0, 1, 3, 2)\n",
    "        \n",
    "        if not mag_only:\n",
    "            complex_mu, complex_sigma = self.complex_decoder(out_5)\n",
    "            complex_out = self.complex_decoder.sample(complex_mu, complex_sigma)\n",
    "            final_real = mag_real + complex_out[:, 0, :, :].unsqueeze(1)\n",
    "            final_imag = mag_imag + complex_out[:, 1, :, :].unsqueeze(1)\n",
    "            return final_real, final_imag\n",
    "        \n",
    "        return mag_real, mag_imag\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78c5ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from pesq import pesq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from utils import LearnableSigmoid\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class LearnableSigmoid(nn.Module):\n",
    "    def __init__(self, in_features, beta=1):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.slope = nn.Parameter(torch.ones(in_features))\n",
    "        self.slope.requiresGrad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.beta * torch.sigmoid(self.slope * x)\n",
    "\n",
    "def pesq_loss(clean, noisy, sr=16000):\n",
    "    try:\n",
    "        pesq_score = pesq(sr, clean, noisy, \"wb\")\n",
    "    except:\n",
    "    #    # error can happen due to silent period\n",
    "        pesq_score = -1\n",
    "    return pesq_score\n",
    "\n",
    "\n",
    "def batch_pesq(clean, noisy):\n",
    "    #pesq_score = Parallel(n_jobs=-1)(\n",
    "    #    delayed(pesq_loss)(c, n) for c, n in zip(clean, noisy)\n",
    "    #)\n",
    "    pesq_score = []\n",
    "    for c,n in zip(clean, noisy):\n",
    "        pesq = pesq_loss(c, n)\n",
    "        pesq_score.append(pesq)\n",
    "    #Mask invalid pesq scores\n",
    "    score_mask = np.array([1 if pqs > -1 else 0 for pqs in pesq_score])\n",
    "    pesq_score = np.array(pesq_score)\n",
    "    pesq_score = (pesq_score - 1) / 3.5\n",
    "    return torch.FloatTensor(score_mask), torch.FloatTensor(pesq_score)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, in_channel=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(in_channel, ndf, (4, 4), (2, 2), (1, 1), bias=False)\n",
    "            ),\n",
    "            nn.InstanceNorm2d(ndf, affine=True),\n",
    "            nn.PReLU(ndf),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(ndf, ndf * 2, (4, 4), (2, 2), (1, 1), bias=False)\n",
    "            ),\n",
    "            nn.InstanceNorm2d(ndf * 2, affine=True),\n",
    "            nn.PReLU(2 * ndf),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(ndf * 2, ndf * 4, (4, 4), (2, 2), (1, 1), bias=False)\n",
    "            ),\n",
    "            nn.InstanceNorm2d(ndf * 4, affine=True),\n",
    "            nn.PReLU(4 * ndf),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(ndf * 4, ndf * 8, (4, 4), (2, 2), (1, 1), bias=False)\n",
    "            ),\n",
    "            nn.InstanceNorm2d(ndf * 8, affine=True),\n",
    "            nn.PReLU(8 * ndf),\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.utils.spectral_norm(nn.Linear(ndf * 8, ndf * 4)),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.PReLU(4 * ndf),\n",
    "            nn.utils.spectral_norm(nn.Linear(ndf * 4, 1)),\n",
    "            LearnableSigmoid(1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        return self.layers(xy)\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a3c9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(src_state_dict, target, get_keys_only=False):\n",
    "    \"\"\"\n",
    "    Copy weights from src model to target model.\n",
    "    Only common layers are transferred.\n",
    "    ARGS:\n",
    "        src_state_dict : source model state dict to copy weights from.\n",
    "        target         : model to copy weights to.\n",
    "\n",
    "    Returns:\n",
    "        A list of layers that were copied.\n",
    "    \"\"\"\n",
    "    src_layers = src_state_dict\n",
    "    target_layers = target.state_dict()\n",
    "    copied_keys = []\n",
    "    for src_key, target_key in zip(src_layers, target_layers):\n",
    "        #If key is empty, it's a description of the entire model, skip this key\n",
    "        if len(src_key) == 0:\n",
    "            continue\n",
    "        #Found a matching key, copy the weights\n",
    "        elif src_key == target_key : \n",
    "            target_layers[target_key].data.copy_(src_layers[src_key].data)\n",
    "            copied_keys.append(target_key)\n",
    "    \n",
    "    #update the state dict of the target model\n",
    "    if not get_keys_only:\n",
    "        target.load_state_dict(target_layers)\n",
    "    \n",
    "    return copied_keys, target\n",
    "        \n",
    "\n",
    "def freeze_layers(model, layers):\n",
    "    \"\"\"\n",
    "    Freezes specific layers of the model.\n",
    "    ARGS:\n",
    "        model : instance of the model.\n",
    "        layer : list of name of the layers to be froze.\n",
    "    \n",
    "    Returns:\n",
    "        Model instance with frozen parameters.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        for layer in layers:\n",
    "            if ((layer == name) or (layer in name)) and param.requires_grad:\n",
    "                param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "class FrameLevelTrainer:\n",
    "    def __init__(self, train_ds, test_ds, win_len, samples, batchsize, pretrain, log_wandb=False, magnitude_only=False, parallel=False, gpu_id=None, pretrain_init=False, resume_pt=False):\n",
    "        self.n_fft = 400\n",
    "        self.hop = 100\n",
    "        self.train_ds = train_ds\n",
    "        self.test_ds = test_ds\n",
    "        self.win_len=win_len\n",
    "        self.samples = samples\n",
    "        self.model = TSCNet(num_channel=64, \n",
    "                            num_features=self.n_fft // 2 + 1, \n",
    "                            win_len=self.win_len, \n",
    "                            gpu_id=gpu_id,\n",
    "                            mag_only=magnitude_only)\n",
    "        self.batchsize = batchsize\n",
    "        self.mag_only = magnitude_only\n",
    "        self.log_wandb = log_wandb\n",
    "      \n",
    "        self.discriminator = Discriminator(ndf=16)\n",
    "\n",
    "        if pretrain_init:\n",
    "            #Load checkpoint\n",
    "            print(f\"Loading pretrained model saved at {args.ckpt}...\")\n",
    "            cmgan_state_dict = torch.load(pretrain, map_location=torch.device('cpu'))\n",
    "            #Copy weights and freeze weights which are copied\n",
    "            keys, self.model = copy_weights(cmgan_state_dict, self.model)\n",
    "            self.model = freeze_layers(self.model, keys)\n",
    "            #Free mem\n",
    "            del cmgan_state_dict   \n",
    "        else:\n",
    "            cmgan_state_dict = torch.load(pretrain, map_location=torch.device('cpu'))\n",
    "            #Get the keys which are supposed to be frozen\n",
    "            keys, _ = copy_weights(cmgan_state_dict, self.model, get_keys_only=True)\n",
    "            self.model = freeze_layers(self.model, keys)\n",
    "            #Free mem\n",
    "            del cmgan_state_dict\n",
    "\n",
    "        #optimizers and schedulers\n",
    "        self.optimizer = torch.optim.AdamW(filter(lambda layer:layer.requires_grad,self.model.parameters()), \n",
    "                                           lr=5e-4)\n",
    "        \n",
    "        self.optimizer_disc = torch.optim.AdamW(\n",
    "            self.discriminator.parameters(), lr=2 * 5e-4\n",
    "        )\n",
    "        self.scheduler_G = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=30, gamma=0.5\n",
    "        )\n",
    "        self.scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer_disc, step_size=30, gamma=0.5\n",
    "        )\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        if resume_pt is not None:\n",
    "            if not resume_pt.endswith('.pt'):\n",
    "                raise ValueError(\"Incorrect path to the checkpoint..\")\n",
    "            self.start_epoch = int(resume_pt.split('.')[0][-1])\n",
    "            self.load_checkpoint(resume_pt)\n",
    "\n",
    "        if gpu_id is not None:\n",
    "            self.model = self.model.to(gpu_id)\n",
    "            self.discriminator = self.discriminator.to(gpu_id)\n",
    "            if parallel:\n",
    "                self.model = DDP(self.model, device_ids=[gpu_id])\n",
    "                self.discriminator = DDP(self.discriminator, device_ids=[gpu_id])\n",
    "        self.gpu_id = gpu_id\n",
    "        \n",
    "    def load_checkpoint(self, path):\n",
    "        try:\n",
    "            state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "            self.model.load_state_dict(state_dict['generator_state_dict'])\n",
    "            self.discriminator.load_state_dict(state_dict['discriminator_state_dict'])\n",
    "            self.optimizer.load_state_dict(state_dict['optimizer_G_state_dict'])\n",
    "            self.optimizer_disc.load_state_dict(state_dict['optimizer_D_state_dict'])\n",
    "            self.scheduler_G.load_state_dict(state_dict['scheduler_G_state_dict'])\n",
    "            self.scheduler_D.load_state_dict(state_dict['scheduler_D_state_dict'])\n",
    "            print(f\"Loaded checkpoint saved at {path} starting at epoch {self.start_epoch}\")\n",
    "            del state_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            state_dict = torch.load(path, map_location=torch.device('cpu'))\n",
    "            \n",
    "            gen_state_dict = OrderedDict()\n",
    "            for name, params in state_dict['generator_state_dict'].items():\n",
    "                name = name[7:]\n",
    "                gen_state_dict[name] = params        \n",
    "            self.model.load_state_dict(gen_state_dict)\n",
    "            del gen_state_dict\n",
    "            \n",
    "            disc_state_dict = OrderedDict()\n",
    "            for name, params in state_dict['discriminator_state_dict'].items():\n",
    "                name = name[7:]\n",
    "                disc_state_dict[name] = params\n",
    "            self.discriminator.load_state_dict(disc_state_dict)\n",
    "            del disc_state_dict\n",
    "            \n",
    "            self.optimizer.load_state_dict(state_dict['optimizer_G_state_dict'])\n",
    "            self.optimizer_disc.load_state_dict(state_dict['optimizer_D_state_dict'])\n",
    "            self.scheduler_G.load_state_dict(state_dict['scheduler_G_state_dict'])\n",
    "            self.scheduler_D.load_state_dict(state_dict['scheduler_D_state_dict'])\n",
    "            \n",
    "            print(f\"Loaded checkpoint saved at {path} starting at epoch {self.start_epoch}\")\n",
    "            del state_dict\n",
    "         \n",
    "    def save_model(self, path_root, exp, epoch, pesq):\n",
    "        \"\"\"\n",
    "        Save model at path_root\n",
    "        \"\"\"\n",
    "        checkpoint_prefix = f\"{exp}_PESQ_{pesq}_epoch_{epoch}.pt\"\n",
    "        path = os.path.join(path_root, checkpoint_prefix)\n",
    "        if self.gpu_id == 0:\n",
    "            save_dict = {'generator_state_dict':self.model.module.state_dict(), \n",
    "                        'discriminator_state_dict':self.discriminator.module.state_dict(),\n",
    "                        'optimizer_G_state_dict':self.optimizer.state_dict(),\n",
    "                        'optimizer_D_state_dict':self.optimizer_disc.state_dict(),\n",
    "                        'scheduler_G_state_dict':self.scheduler_G.state_dict(),\n",
    "                        'scheduler_D_state_dict':self.scheduler_D.state_dict(),\n",
    "                        'epoch':epoch,\n",
    "                        'pesq':pesq\n",
    "                        }\n",
    "            \n",
    "            torch.save(save_dict, path)\n",
    "            print(f\"checkpoint:{checkpoint_prefix} saved at {path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b089a83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint saved at /Users/anuragkumar/Anurag/CMGAN/src/best_ckpt/CMGAN_frame/CMGAN_win_25_PESQ_1.9905052185058594_epoch_4.pt starting at epoch 1\n"
     ]
    }
   ],
   "source": [
    "trainer = FrameLevelTrainer(train_ds=None, \n",
    "                            test_ds=None, \n",
    "                            win_len=25, \n",
    "                            samples=None, \n",
    "                            batchsize=2, \n",
    "                            log_wandb=False, \n",
    "                            magnitude_only=False, \n",
    "                            parallel=False, \n",
    "                            gpu_id=None, \n",
    "                            pretrain=\"/Users/anuragkumar/Anurag/CMGAN/src/best_ckpt/ckpt\", \n",
    "                            pretrain_init=False, \n",
    "                            resume_pt=\"/Users/anuragkumar/Anurag/CMGAN/src/best_ckpt/CMGAN_frame/CMGAN_win_25_PESQ_1.9905052185058594_epoch_4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca14e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf-debug",
   "language": "python",
   "name": "rlhf-debug"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
